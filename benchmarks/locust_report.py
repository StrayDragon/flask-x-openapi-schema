#!/usr/bin/env python
"""
Analyze locust benchmark results and generate a rich table report.

This script:
1. Reads the CSV files generated by locust
2. Analyzes the results
3. Displays a rich table report with performance metrics
"""

import csv
from pathlib import Path
from typing import Dict

from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich import box

# Initialize rich console
console = Console()


def parse_locust_results(stats_file: str) -> Dict:
    """Parse the Locust stats CSV file and return the results."""
    results = {
        "standard": {
            "requests": 0,
            "failures": 0,
            "median_response_time": 0,
            "avg_response_time": 0,
            "min_response_time": 0,
            "max_response_time": 0,
        },
        "openapi": {
            "requests": 0,
            "failures": 0,
            "median_response_time": 0,
            "avg_response_time": 0,
            "min_response_time": 0,
            "max_response_time": 0,
        },
    }

    # Check if the file exists
    if not Path(stats_file).exists():
        console.print(f"[bold red]Results file not found:[/bold red] {stats_file}")
        console.print("[yellow]Using default values for benchmark results.[/yellow]")
        return results

    try:
        with open(stats_file, newline="") as csvfile:
            reader = csv.DictReader(csvfile)

            # Check if the file is empty or has no rows
            rows = list(reader)
            if not rows:
                console.print(
                    "[bold yellow]Results file is empty or has no data rows.[/bold yellow]"
                )
                return results

            for row in rows:
                # Skip the aggregated row
                if row.get("Name", "") == "Aggregated":
                    continue

                # Determine if this is a standard or openapi request
                name = row.get("Name", "")
                if "standard" in name.lower():
                    category = "standard"
                elif "openapi" in name.lower():
                    category = "openapi"
                else:
                    continue

                # Update the results, handling missing or invalid values
                try:
                    results[category]["requests"] += int(row.get("Request Count", 0))
                except (ValueError, TypeError):
                    pass

                try:
                    results[category]["failures"] += int(row.get("Failure Count", 0))
                except (ValueError, TypeError):
                    pass

                # Always update response times, even if there are failures
                # This gives us performance data even when endpoints are failing
                try:
                    # Update median response time (weighted average if multiple endpoints)
                    median_time = float(row.get("Median Response Time", 0))
                    if results[category]["median_response_time"] == 0:
                        results[category]["median_response_time"] = median_time
                    else:
                        results[category]["median_response_time"] = (
                            results[category]["median_response_time"] + median_time
                        ) / 2
                except (ValueError, TypeError):
                    pass

                try:
                    # Update average response time (weighted average if multiple endpoints)
                    avg_time = float(row.get("Average Response Time", 0))
                    if results[category]["avg_response_time"] == 0:
                        results[category]["avg_response_time"] = avg_time
                    else:
                        results[category]["avg_response_time"] = (
                            results[category]["avg_response_time"] + avg_time
                        ) / 2
                except (ValueError, TypeError):
                    pass

                try:
                    # Update min response time
                    min_time = float(row.get("Min Response Time", 0))
                    if (
                        results[category]["min_response_time"] == 0
                        or min_time < results[category]["min_response_time"]
                    ):
                        results[category]["min_response_time"] = min_time
                except (ValueError, TypeError):
                    pass

                try:
                    # Update max response time
                    max_time = float(row.get("Max Response Time", 0))
                    if max_time > results[category]["max_response_time"]:
                        results[category]["max_response_time"] = max_time
                except (ValueError, TypeError):
                    pass
    except Exception as e:
        console.print(f"[bold red]Error parsing Locust results:[/bold red] {e}")
        console.print("[yellow]Using available data for benchmark results.[/yellow]")

    return results


def display_results(results: Dict) -> None:
    """Display the benchmark results using rich tables."""
    # Create the main results table
    table = Table(title="Locust Benchmark Results", box=box.ROUNDED)
    table.add_column("Metric", style="cyan")
    table.add_column("Standard Flask", style="yellow")
    table.add_column("Flask-X-OpenAPI-Schema", style="green")
    table.add_column("Difference", style="magenta")

    # Add rows for each metric
    metrics = [
        ("Requests", "requests", ""),
        ("Failures", "failures", ""),
        ("Median Response Time", "median_response_time", "ms"),
        ("Average Response Time", "avg_response_time", "ms"),
        ("Min Response Time", "min_response_time", "ms"),
        ("Max Response Time", "max_response_time", "ms"),
    ]

    for display_name, key, unit in metrics:
        standard_value = results["standard"][key]
        openapi_value = results["openapi"][key]

        # Calculate difference and percentage
        if key in ["requests", "failures"]:
            diff = openapi_value - standard_value
            diff_percent = (diff / standard_value * 100) if standard_value > 0 else 0
            table.add_row(
                display_name,
                f"{standard_value}",
                f"{openapi_value}",
                f"{diff} ({diff_percent:.2f}%)",
            )
        else:
            diff = openapi_value - standard_value
            diff_percent = (diff / standard_value * 100) if standard_value > 0 else 0
            table.add_row(
                display_name,
                f"{standard_value:.2f}{unit}",
                f"{openapi_value:.2f}{unit}",
                f"{diff:.2f}{unit} ({diff_percent:.2f}%)",
            )

    console.print(table)

    # Check if there were failures
    standard_failures = results["standard"]["failures"]
    openapi_failures = results["openapi"]["failures"]

    if openapi_failures > 0 or standard_failures > 0:
        console.print(
            Panel(
                f"[bold yellow]Warning: Some requests failed during the benchmark.[/bold yellow]\n\n"
                f"- Standard Flask failures: [yellow]{standard_failures}[/yellow] out of {results['standard']['requests']} requests\n"
                f"- Flask-X-OpenAPI-Schema failures: [yellow]{openapi_failures}[/yellow] out of {results['openapi']['requests']} requests\n\n"
                "The response time metrics still provide useful information about the relative performance,\n"
                "but the absolute values may be affected by the failures.",
                title="Failure Warning",
                border_style="yellow",
                padding=(1, 2),
            )
        )

    # Calculate and display overhead
    if results["standard"]["avg_response_time"] > 0:
        overhead = (
            (
                results["openapi"]["avg_response_time"]
                - results["standard"]["avg_response_time"]
            )
            / results["standard"]["avg_response_time"]
            * 100
        )

        console.print(
            Panel(
                f"[bold]Flask-X-OpenAPI-Schema adds a [red]{overhead:.2f}%[/red] overhead compared to standard Flask.[/bold]\n\n"
                "However, this overhead is negligible in absolute terms:\n"
                f"- Standard Flask: [yellow]{results['standard']['avg_response_time']:.2f}ms[/yellow] per request\n"
                f"- Flask-X-OpenAPI-Schema: [green]{results['openapi']['avg_response_time']:.2f}ms[/green] per request\n"
                f"- Absolute difference: [magenta]{results['openapi']['avg_response_time'] - results['standard']['avg_response_time']:.2f}ms[/magenta]\n\n"
                "For comparison:\n"
                "- Network latency: 10-100ms\n"
                "- Database queries: 1-10ms\n"
                "- Rendering templates: 5-20ms",
                title="Performance Analysis",
                border_style="cyan",
                padding=(1, 2),
            )
        )

    # Display benefits
    console.print(
        Panel(
            "[bold]Benefits of using Flask-X-OpenAPI-Schema:[/bold]\n\n"
            "1. [green]Automatic Validation[/green]: No need to write manual validation code\n"
            "2. [green]Type Safety[/green]: Leverage Python's type system for better code quality\n"
            "3. [green]Self-Documenting APIs[/green]: Automatically generate OpenAPI documentation\n"
            "4. [green]Reduced Boilerplate[/green]: Write less code for the same functionality\n"
            "5. [green]Better Maintainability[/green]: Cleaner, more structured code\n"
            "6. [green]Consistent Error Handling[/green]: Standardized error responses\n"
            "7. [green]IDE Support[/green]: Better autocompletion and type checking\n\n"
            "[bold]The real value is in developer productivity and code quality,[/bold]\n"
            "[bold]not in raw performance.[/bold]",
            title="Why Use Flask-X-OpenAPI-Schema?",
            border_style="green",
            padding=(1, 2),
        )
    )


def main() -> None:
    """Analyze locust benchmark results and display a report."""
    console.print(
        Panel.fit(
            "[bold cyan]Flask-X-OpenAPI-Schema Locust Benchmark Report[/bold cyan]\n\n"
            "Analyzing results from locust benchmark",
            title="Benchmark Report",
            border_style="cyan",
            padding=(1, 2),
        )
    )

    try:
        # Parse the results
        results = parse_locust_results("benchmarks/results_stats.csv")

        # Display the results
        display_results(results)
    except KeyboardInterrupt:
        console.print(
            "[bold yellow]Report generation interrupted by user.[/bold yellow]"
        )
    except Exception as e:
        console.print(f"[bold red]Error generating report:[/bold red] {e}")


if __name__ == "__main__":
    main()
